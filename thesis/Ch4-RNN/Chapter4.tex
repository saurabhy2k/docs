\graphicspath{{./Ch4-RNN/images/}}

\chapter{Optimizing the Performance of RNN/LSTM Accelerators} \label{chap:RNN}
\section{Introduction}
Many applications involve sequential data processing and time-series predictions, e.g., natural language processing, speech recognition, music composition, and video activity recognition. As convolution neural networks (CNNs) are specialized for processing image data, recurrent neural networks (RNNs) are specialized in handling sequential data. Processing sequential data requires remembering the contextual information from previous data. Recurrent neural networks (RNNs) are specialized in handling such problems by maintaining an internal state based on previously seen data. RNNs scale well with long sequences and even sequences of variable lengths. They share weights across different time steps. LSTMs \cite{hochreiter1997long} are variants of RNNs designed to handle long-range dependencies by storing useful information about previous inputs for a long duration. 

LSTM computations involve several large matrix-vector multiplications, which are performed for many time steps. The inputs to the network are a time sequence of vectors, and these large matrices hold weights, which are learned during the training process. The sizes of these matrices can be significant in several MBs and often exceed the size of the accelerator's on-chip memory. These matrices are partitioned into blocks and accessed from off-chip memory repeatedly by the accelerator, which results in a large volume of off-chip memory accesses and energy consumption.
\section{Background}
LSTM has recurrent connections to capture the long and short-term dependencies. LSTM cells maintain the cell state to store the dependency information derived from the previously seen data and use four gates to modify the cell state and produce the output. Typically the computations of LSTM cell is described by the following equations
%LSTM has recurrent connections to capture the long and short-term dependencies. Typically the computations of LSTM cell is described by the following equations
\begin{align}\label{eq:lstmEqs}
	\begin{split}
		&i{=}{\sigma}(W^i{\cdot}x_t{+}R^i{\cdot}h_{t-1}{+}b^i)\\
		&f{=}{\sigma}(W^f{\cdot}x_t{+}R^f{\cdot}h_{t-1}{+}b^f)\\
		&g{=}{\tanh}(W^g{\cdot}x_t{+}R^g{\cdot}h_{t-1}{+}b^g)\\
		&o{=}{\sigma}(W^o{\cdot}x_t{+}R^o{\cdot}h_{t-1}{+}b^o)\\
		&c_{t}{=}f{\odot}c_{t-1}{+}i{\odot}g\\
		&h_{t}{=}o{\odot}{\tanh}(c_t)
	\end{split}	
\end{align}
where $x_t$ is the input, $h_t$ is the hidden state and $c_t$ is the cell state at time $t$. $i,f,g,o$ are the computed gate values. $\odot$ denotes the element-wise multiplications. $W^j$ and $R^j$ are the input and hidden state weight matrices, respectively, and $b^j$ is the bias vector, where $j\in\{i,f,o,g\}$. $W^j$, $R^j$, and $b^j$ are the parameters learned during the training process. Once the network is trained, these parameters are used during inferencing. 
If the dimensions of the input vector $x_t$ is $L$ and that of the hidden state vector $h_t$ is $N$, the dimensions of $W^j$, $R^j$ and $b^j$ are $N{\times}L$, $N{\times}N$ and $N$, respectively. $N$ is the number of hidden states of the LSTM. 

\eqref{eq:lstmEqs} involves matrix-vector multiplications and element-wise operations. Element-wise operations are vector-vector additions, multiplications, and non-linear functions. The non-linear functions are hyper-tangent ($\tanh$) and sigmoid ($\sigma$).
\begin{figure}[!htb]
	\centerline{\includegraphics[width=0.7\textwidth]{dataDependencyConventional.pdf}}
	\caption{Data dependency in computations of LSTMs between consecutive time steps}
	\label{fig:lstmComputationConv}
\end{figure}
LSTM computations (~\eqref{eq:lstmEqs}) for two consecutive time steps are shown in~\figref{fig:lstmComputationConv}. At every time step,~\eqref{eq:lstmEqs} take vectors $x_t$ as input and compute the cell state ($c_t$) and hidden state ($h_t$) using the previous hidden state ($h_{t-1}$) and cell state ($c_{t-1}$) vectors. $h_t$ depends on the present input vector $x_t$ and the previous time step cell state ($c_{t-1}$) and hidden state ($h_{t-1}$) vectors. The dependency of $h_t$ on $h_{t-1}$ and $c_{t-1}$ prevents the parallel processing of multiple time steps. 

LSTM accelerators have small on-chip memory. The large weight matrices $R$ and $W$ are stored in off-chip memory. The dependency of $h_{t}$ and $c_{t}$ on the previous time step computations makes the reuse of weight matrix $R$ a challenge. Thus the weights are accessed from the off-chip memory at every step, resulting in sizeable off-chip memory accesses and high energy consumption of these accelerators. This work focuses on reducing the off-chip memory accesses of $R$ during the LSTM inference phase. 

Our approach splits the computations of a time step and computes the partial sums of two consecutive time steps. As shown in \figref{fig:dataDependencyProposed}, the proposed approach at time step t computes $h_t$ using the input partial sum $S^U_t$, and partial sum computed at current time step $S^L_t$. $h_t$ is then used to compute the partial sum $S^L_{t{+}1}$ for next time step, while reusing the weights of $R$. $S^L_{t{+}1}$ is then passed for the next step computations of $h_{t{+}1}$. In our approach, only half of the matrix $R$ is accessed at each time step, reducing the $R$ matrix accesses by half.
\begin{figure}[!htb]
	\centerline{\includegraphics[width=0.85\textwidth]{dataDependencyProposed.pdf}}
	\caption{Proposed approach: Splitting computations into partial sums and reusing the weights of $R$.}
	\label{fig:dataDependencyProposed}
\end{figure}
\section{Related Work}
To address the computational and energy efficiency of LSTMs and, in general RNNs, several ASIC~\cite{conti2018chipmunk,wang2017accelerating,azari2020elsa} and FPGA based accelerators~\cite{chang2015recurrent,ferreira2016fpga,lee2016fpga,guan2017fpga,han2017ese} are proposed. The energy efficiency of LSTM accelerators is critical for their widespread usage, and off-chip memory access is the key to improving energy consumption. Most of these works focused on improving energy efficiency by reducing off-chip memory accesses.

Some approaches~\cite{lee2016fpga, rybalkin2018finn, ferreira2016fpga} used on-chip memory to store all the weights. Sizes of weights in recent multi-layer LSTM models can be several MB's, and using large on-chip memory is expensive. These approaches are not scalable and effective only for small LSTM models. Our approach is independent of model size and effective for large LSTM models.

Several approaches used the fact that neural networks are error-tolerant and have lots of redundancy. They used quantization and pruning techniques to compress the models' size. Approaches~\cite{ferreira2016fpga,wang2018c} used 18-bit, Chang et al.~\cite{chang2015recurrent} used 16-bit, Han et al.~\cite{han2017ese} used 12-bit precision for storing the inputs and weights, Lee et al.~\cite{lee2016fpga} used 8-bit inputs and 6-bits for weights to reduce the model size. Our approach is orthogonal to the quantization techniques and can be integrated with different quantization techniques to reduce memory access further. 

Han et al.~\cite{han2017ese} used pruning to compress the model. However, pruning results in irregular network structure and the sparse matrices require additional computational and storage resources resulting in unbalanced load distribution. To overcome this, Wang et al.~\cite{wang2018c} used block-circulant matrices representations to compress the LSTM/RNN model and eliminate irregularities resulting from compression. Some approaches~\cite{park2019balancing,han2017ese,park2018maximizing} used load balance aware pruning techniques to overcome the unbalanced load distribution problem. 

Quantization and pruning approaches compromise the accuracy of the networks. The other line of work reduced the memory accesses without affecting the accuracy of the NNs output by applying the data-reuse techniques. The matrix-vector multiplication $W^j{\cdot}x$ in~\eqref{eq:lstmEqs}, where $j\in \{i,f,g,o\}$, is independent of previous state computation. Que et al.~\cite{que2019efficient} proposed a blocking-batching scheme that reuses the weights of $W^j$ matrix by processing a group of input vectors as a batch. The input vectors in the same batch share the same weight matrices ($W^j$). However, it is difficult to collect the required number of input vectors. As the LSTM cell states ($h_t$ and $c_t$) computations depend on previous time-step cell states, the benefit of their batching schemes is limited to $W^j{\cdot}x$. Reusing weights of $R$ across different time steps has not been successful because of the dependency on previous time-step states.

Park et al.(~\cite{park2020time}) proposed a time-step interleaved weight reuse scheme (TSI-WR) which reuses the weights of $R$ matrix between two adjacent time steps by performing computations in a time-interleaved manner. Their approach logically partitions the $R$ matrix into blocks. A block is accessed from off-chip memory to compute the hidden state vector $h_t$, and a fraction of it is reused to compute the partial sum of next time step state $h_{t+1}$. However, their approach only partially exploits the data reuse, and several weights are accessed repeatedly from the off-chip memory. In addition, the data reuse in the TSI-WR approach depends on the on-chip storage size, which benefits accelerators with larger on-chip memory.

Our approach schedules the computations in a way that reuses all the weights of $R$ between two adjacent time steps. The data reuse in our approach is independent of on-chip buffer sizes, which even benefits to accelerators with small on-chip memory. 

\section{Split And Combine Computations Approach}
In this section, we first describe the basic idea of the Split And Combine Computations (\textbf{SACC}) approach and then its extension to block-wise data reuse.
\subsection{Basic Approach}\label{sec:elementWiseApproach}
The computation of $h_t$ can be expressed as follows:
\begin{align}\label{eq:h_{t}}
	h_{t}[k] &= F( S_{t}[k]+q_{t}[k])
\end{align}
Where $F$ is a non-linear function, and $q_{t}$ is computed as $W{\cdot}x_t{+}b$. The computations of $q_{t}$ are independent of the previous step's cell states. $S_{t}[k]$ is the sum of $N$ product terms, given by:
\begin{align}\label{eq:S_{t}}
	S_{t}[k] = \sum_{n=0}^{N-1}R[k][n]\cdot h_{t-1}[n]
\end{align}
Computation of $S_{t}[k]$ (and, consequently, $h_{t}[k]$) depends on all $N$ elements of $h_{t-1}$. Traditional approaches compute all $N$ elements of $h_{t-1}$ first, requiring access to the full matrix $R$, before initiating computations for $h_{t}$. However, due to limited on-chip memory, elements of matrix $R$ are replaced by other elements required for $h_{t}$ computations, and when the accelerator starts computing $h_{t+1}$, the weights of $R$ are reaccessed from the off-chip memory.

To enable reuse of the weights of $R$, we split the computations of $S_{t}[k]$ into two partial sums as follows:
\begin{align}\label{eq:S_{t}_split}
	S_{t}[k] = \sum_{n=0}^{k}R[k][n]\cdot h_{t-1}[n] + \sum_{n=k+1}^{N-1}R[k][n]\cdot h_{t-1}[n]
\end{align}
We define $S_{t}^{L}[k]$ and $S_{t}^{U}[k]$ as the first and second partial sums in the above equation, respectively. They are expressed as follows: 
\begin{align}
	S_{t}^{L}[k] &= \sum_{n=0}^{k}R[k][n]\cdot h_{t-1}[n] \label{eq:S_L_{t}}\\
	S_{t}^{U}[k] &= \sum_{n=k{+}1}^{N-1}R[k][n]\cdot h_{t-1}[n] \label{eq:S_U_{t}}
\end{align}
Here, $S_{t}^{L}[k]$ uses the lower-diagonal and diagonal elements of $R$ ($R^L$), while $S_{t}^{U}[k]$ uses the upper-diagonal elements of $R$ ($R^U$). The elements of $R$ used in the computations of $S_{t}$ are also required for the partial sum computations of $S_{t+1}$.
In our approach $S_{t{+}1}^{L}[k]$ computations can be initated soon after first $k{+}1$ elements of $h_{t}$ are computed. Similarly at the next time step, the computations of $S^U_{t{+}2}[k]$ can be initiated soon after the last $N{-}(k{+}1)$ elements of $h_{t{+}1}$ are computed. By dividing the computations into two partial sums, we enable the reuse of weights of $R$ between two consecutive time steps. Our approach accesses the weights of $R$ and reuses them for computations of partial sums for two consecutive time steps.

In \figref{fig:LowerDiagR} and \figref{fig:UpperDiagR}, we can observe the elements of matrix $R$ that are utilized in the partial sum computations of $S_{t}^{L}$ and $S_{t}^{U}$, respectively, for an example $3\times3$ matrix $R$. In this case, each element of the resulting vector $S$ is the sum of three product terms. Notably, the partial sum vectors $S^L_t$ and $S^U_t$ only contain a fraction of the total sum $S$, and these fractions are visually highlighted using colored rectangles in \figref{fig:partialSumComputations}.

\begin{figure}[htb!]
	\centering
	\subfloat[]
	{\includegraphics[width=0.3\textwidth]{LowerDiagR.pdf}
		\label{fig:LowerDiagR}}
	\hspace{2.0em}
	\subfloat[]
	{
		\includegraphics[width=0.3\textwidth]{UpperDiagR.pdf}
		\label{fig:UpperDiagR}
	}
	\caption{Fraction of partial sums computated using (a) lower diagonal element of $R$  (b) upper diagonal elements of $R$ }
	\label{fig:partialSumComputations}
\end{figure}

An illustrative example that demonstrates the computations performed in two consecutive steps, $t$ and $t{+}1$, is presented in \figurename{~\ref{fig:ExampleComputation_t}} and \figurename{~\ref{fig:ExampleComputation_t+1}} for a $3{\times}3$ matrix $R$. The bold lines in \figurename{~\ref{fig:ExampleComputation_t}} and \figurename{~\ref{fig:ExampleComputation_t+1}} highlight the reuse of weights from matrix $R$ at different stages.
\begin{figure}[htbp]
	\centerline{\includegraphics[width=\textwidth]{LSTMStepStages_t.pdf}}
	\caption{Computation stages at time step $t$.}
	\label{fig:ExampleComputation_t}
\end{figure}

At time step $t$, $R^L$ is accessed and there are $\frac{N{\times}(N{+}1)}{2}$ stages. In this specific example, with $N=3$, we have six stages at time step $t$. In each stage, an element of matrix $R$ is accessed and processed through a Multiply Accumulate Unit (MAC). The processing involves multiplying the element of $R$ with a corresponding element from the vector $h_{t{-}1}$, and the product is accumulated. This process continues until all the elements of a row of matrix $R^L$ have been accessed. Subsequently, the output of the MAC is added to the corresponding element of vector $S^U_{t}$ to compute the element of vector $h_t$, and the MAC unit is then reset to zero. Furthermore, the elements of matrix $R^L$ are efficiently reused to compute the partial sum vector $S^L_{t{+}1}$, utilizing the values of vector $h_{t}$ computed in previous stages or current stage.
\begin{figure}[htbp]
	\centerline{\includegraphics[width=\textwidth]{LSTMStepStages_t1.pdf}}
	\caption{Computation stages at time step $t{+}1$.}
	\label{fig:ExampleComputation_t+1}
\end{figure}

\figurename{~\ref{fig:ExampleComputation_t+1}} illustrates the computation stages for time step $t{+}1$, which utilizes the previously computed $S^L_{t{+}1}$ from time step $t$. At this particular time step, there are $\frac{N{\times}(N{-}1)}{2}$ stages. In this example, with $N=3$, we observe three stages at time step $t{+}1$. For each stage, the inputs are $h_t$ and $S^L_{t{+}1}$, which are carried over from the previous time step $t$. Subsequently, the computation results in the outputs $h_{t{+}1}$ and the partial sum vector $S^U_{t{+}2}$, efficiently reusing the weights of matrix $R^U$.

By analyzing both \figurename{~\ref{fig:ExampleComputation_t}} and \figurename{~\ref{fig:ExampleComputation_t+1}}, we can observe that all the elements of matrix $R$ are accessed once, enabling the computation of the outputs $h_t$ and $h_{t{+}1}$ along with the partial sum $S^U_{t{+}2}$, which is then passed on to the next time step.

\subsection{Block-wise reuse}
The basic approach accesses one element of $R$ at each stage. However, to leverage the LSTM accelerator's capability of storing multiple elements of $R$ and processing multiple elements simultaneously, the basic approach is extended by partitioning $R$ into square matrices. The accelerator's on-chip memory determines these square matrices' size. Specifically, let $R$ be partitioned into square matrices $P$ of size $B{\times}B$, where $B$ is chosen such that $P$ fits into the on-chip memory. Each $P$ matrix is indexed as $(r,m)$, where $0{\le}r,m{\le} (\ceil{\frac{N}{B}}{-}1)$, as illustrated in \figurename{~\ref{fig:squareMatrix}}.

These small matrices are then grouped into two sets, $R^L$ and $R^U$. The set $R^L$ contains all the matrices with index $r{\geq}m$, while $R^U$ contains all the matrices with index $r{<}m$, as depicted in \figurename{~\ref{fig:lowerSquareMatrix}} and \figurename{~\ref{fig:upperSquareMatrix}}, respectively.
\begin{figure}[htb!]
	\centering
	\subfloat[]
	{\includegraphics[width=0.25\textwidth]{squareMatrix.pdf}
		\label{fig:squareMatrix}}
%	\hspace{2.0em}
	\subfloat[]
	{
		\includegraphics[width=0.25\textwidth]{lowerSquareMatrix.pdf}
		\label{fig:lowerSquareMatrix}
	}
	\subfloat[]
	{
		\includegraphics[width=0.245\textwidth]{upperSquareMatrix.pdf}
		\label{fig:upperSquareMatrix}
	}
	\caption{(a) Partitions of a matrix $R_{N{\times}N}$ into $B{\times}B$ square matrices. (b) $R^L{=}\{P_{(r,m)}:r{\geq}m$\} (b) $R^U{=}\{P_{(r,m)}:r{<}m$\}	}
	\label{fig:SquareMatrices}
\end{figure} 

In the block approach, computations of $h_t$ are divided into $\ceil{\frac{N}{B}}$ slices, each containing $B$ elements. 
%\figurename{~\ref{fig:hVectorSlices}} illustrates an example vector $h_t$ of length $N{=}9$, divided into slices of length $B{=}3$. The computations of $r^{th}$ slice of $h_t$ requires all the partitions $P_{r{\times}m}$ with the same row index $r$. The partitions $P$ utilized in the computations of each slice of $h$, are indicated below each slice in \figurename{~\ref{fig:hVectorSlices}}. This approach enables the computed values of $h_t$ to be immediately used for computing the partial sum of the next time step, while efficiently reusing the weights of partitions $P$. 
%\begin{figure}[!htb]
%	\centerline{\includegraphics[width=0.4\textwidth]{hVectorSlices.pdf}}
%	\caption{Slices of vector $h$ and partitions of $R$ required to compute each slice of $h$.}
%	\label{fig:hVectorSlices}
%	\vspace{-1.0em}	
%\end{figure}
For computing the $k^{th}$ element of the $r^{th}$ slice of $h_{t}$, where $0{\le}k{\le}B{-}1$, the following equation is employed:
\begin{align}
	h_{t}[B{*}r{+}k]{=}F(\sum_{m{=}0}^{\ceil{\frac{N}{B}}{-}1}S_{(r,m)}[k]+q_{t}[B{*}r{+}k]) \label{eq:blockWiseh}
\end{align}
where $S_{(r,m)}$ represents the partial sum of the $r^{th}$ slice of $S$, computed using the partition $P_{(r,m)}$. The element $S_{(r,m)}[k]$ is calculated using the equation below:
\begin{align}	 \label{eq:blockWiseSum}
	\begin{split}
	S_{(r,m)}[k]&{=}\sum_{j=0}^{B{-}1}P_{(r,m)}[k][j]\cdot h_{t-1}[B{*}m{+}j]\\
	&{=}\sum_{j=0}^{B{-}1}R[B{*}r{+}k][B{*}m{+}j]\cdot h_{t-1}[B{*}m{+}j]
 \end{split}
\end{align}
The summation $\sum_{m{=}0}^{\ceil{\frac{N}{B}}{-}1}S_{(r,m)}[k]$ in \eqref{eq:blockWiseh} can be broken down into two partial sums:
\begin{align}
	&\sum_{m{=}0}^{\ceil{\frac{N}{B}}{-}1}S_{(r,m)}[k]{=}\sum_{m{=}0}^{r}S_{(r,m)}^{L}[k]{+}\sum_{m{=}r{+}1}^{\ceil{\frac{N}{B}}{-}1}S_{(r,m)}^{U}[k] 
\end{align}
\begin{figure}[!htb]
	\centerline{\includegraphics[width=0.7\textwidth]{blockPartialSum.pdf}}
	\caption{Partitions of $R^L$ and $R^U$ required for computations of partial sum $S^L$ and $S^U$.}
	\label{fig:blockPartialSum}
\end{figure}

\figref{fig:blockPartialSum} further illustrates the partitions $P$ used in computing $\sum_{m{=}0}^{r}S_{(r,m)}^{L}[k]$ and $\sum_{m{=}r{+}1}^{\ceil{\frac{N}{B}}{-}1}S_{(r,m)}^{U}[k]$.
Partial sum computations of the $r^{th}$ slice of the $(t{+}1)^{th}$ time step can be initiated as soon as the first $r$ slices ($B{\times}r$ elements) of $h_t$ are computed. Computations of the $r^{th}$ slice of partial sums ($S_L$ and $S_U$) at different time steps use the same partitions $P$ of $R$. The proposed approach exploits this to effectively reuse the weights of $P$ to compute the partial sums of two consecutive time steps. This optimization ensures efficient data dependency handling and speeds up the overall computation process.

Algorithm~\ref{Algo:SACCAlgo} describes the block wise SACC approach. The dimensions of $W$, $R$, $b$, and $x_t$ are $4N{\times}L$, $4N{\times}N$, $4N{\times}1$, and $L{\times}1$, respectively. The algorithm stores the vectors $h_t$, $c_t$, and the partial sum vectors ($s_{t+1}$) in the on-chip memory and accesses the weights from the off-chip memory. It first computes the vector $q_t$ as $W{\cdot}x{+}b$, at line~\ref{alg:Wx+b} and then invokes the procedures $\textproc{UpDiagReuse}$ at line~\ref{alg:callUpDiaReuse} or  \textproc{LowDiagReuse} at line~\ref{alg:callLowDiaReuse} at alternate time steps. \textproc{LowDiagReuse} accesses blocks of $R^L$, and \textproc{UpDiagReuse} accesses blocks of $R^U$. The procedures have two nested loops. \textproc{LowDiagReuse} traverses the blocks from top to bottom (at line \ref{alg:LowDiag_outFor}), left to the right (at line \ref{alg:LowDiag_inFor}), while the \textproc{UpDiagReuse} traverses the blocks in the opposite order. The inner loop accesses the $(r,m)^{th}$ block of $R$ from the off-chip memory and reuses it to compute the partial sums $s^B_{t+1}$ and $s^B_{t+2}$. The outer loop iterations compute $r^{th}$ slices of $h_{t+1}$, $c_{t+1}$, and $s_{t+2}$.
When all the blocks of the $r^{th}$ row are processed, $s^B_{t+1}$ has the total sum, which is then used to compute the $r^{th}$ slice of the vectors $h_{t+1}$ and $c_{t+1}$ using \textproc{LSTMEquations} at line~\ref{alg:LowDiag_LstmEq}. 
Both the procedures reuse the blocks of $R$ to reduce the off-chip memory accesses. Algorithm~\ref{Algo:LSTMEq} implements the LSTM equations using the partial sum vector.
\input{algoWithBlockReuse}
\input{algoLSTMEqns}
\section{Experimental Setup And Results}
We have implemented LSTM layers using the conventional, TSIWR~\cite{park2020time}, and our approach and synthesized the design using the SDSoC framework, SDx v2018.3. 
\figurename{~\ref{fig:lstmFpgaDesign} shows the main components and interfaces of the FPGA design. The design reads $B{\times}B$ size blocks of the four input $R$ matrices ($R_i, R_f, R_g$, and $R_o$) into the local buffers using bufferCopy component and then performs the matrix-vector multiplications using $MxV$ component. The blocks are read in the order specified by the schedule. The schedule for all three approaches is prepared offline. The top-level logic first reads the blocks of $R$ matrices ($R_i, R_f, R_g$, and $R_o$) and invokes computeMxV to compute the four matrix-vector multiplications in parallel. The design uses the dual buffering scheme to overlap the computations with data transfer. The four matrix multiplication units ($MxV_i, MxV_f, MxV_g$, and $MxV_o$) perform matrix-vector multiplications in parallel, and the amount of parallelism in each $MxV$ unit can be configured. 
\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.65\columnwidth]{SACC_FPGA_Design}
	\caption{Block level FPGA Design for LSTM Implementation.}
	\label{fig:lstmFpgaDesign}
\end{figure}

The design can be configured for different on-chip buffer sizes, hidden units ($N$), and compute resources. We have implemented multiple designs for each of the three approaches to compare the throughput for different numbers of compute resources and buffer sizes. We carried out the experiments on Zedboard, and the target frequency is 100MHz. The off-chip memory is DDR3 connected using a 64-bit AXI bus. We have integrated the Xilinx AXI Performance Monitor (APM) IP to log the number of bytes transferred and memory access latencies for DRAM accesses.

There are eight local buffers each to store the $B{\times}B$ elements of the matrices (\figurename{~\ref{fig:lstmFpgaDesign}}), and two buffers, each of dimension $N$, for storing the vector $h_t$ and $h_{t{+}1}$. Table~\ref{tab:fpgaResources} shows the FPGA resources utilization reported by Vivado for conventional, TSIWR, and our approach, for $B{=}64$ and using 8 DSPs resources ($PARL\_FACTOR{=}8$) in each of the four $MxV$ component. In this case, the total number of DSPs required is $4{\times}PARL\_FACTOR{=}32$.
Each of the eight arrays is partitioned into 8 ($PARL\_FACTOR$) to provide the data to compute resources. That results in 64 FPGA RAMB18 resources for storing the $B{\times}B$ blocks and 2 FPGA RAMB18 resources for storing the vector $h_t$ and $h_{t{+}1}$. The remaining RAMB18 blocks are used for other logic and vary for the three approaches. The conventional approach has the most straightforward control logic than TSIWR and our approach. The LUTs and FFs in the case of TSIWR and our approach are higher than the conventional approach. 
\begin{table}[htb]
	\centering
	\caption{FPGA resource utilization for B=64, PARL\_FACTOR=8, and N=128 }
	\label{tab:fpgaResources}
	\begin{tabular}{ccccc}
		\hline
		& RAMB18 & DSPs & LUTs  & FFs   \\ \hline
		Conv. & 86     & 32   & 14107 & 17768 \\ \hline
		TSIWR & 86     & 32   & 16669 & 22465 \\ \hline
		SACC  & 86     & 32   & 15738 & 18571 \\ \hline
	\end{tabular}
\end{table}
\subsection{Baseline}
We have compared our approach (SACC) with conventional and TSI-WR~\cite{park2020time} approaches. In our experiments, to perform a fair comparison, the size of the on-chip memory, matrix multiplication logic, and number of compute resources are kept the same for all three approaches.

We have also implemented the models to compute the off-chip memory accesses for all three approaches and integrated them with the analytical framework (Chapter~\ref{chap:analyticalFw}) to compare the off-chip memory accesses of the three approaches.
\subsection{Benchmarks}
To demonstrate the efficiency of our approach, we have experimented with LSTM models used in speech recognition (for TIMIT~\cite{garofolo1993timit}) and character level Language Modelling (LM)~\cite{sundermeyer2015feedforward}. These models are widely used in natural language processing. The LSTM models are adopted from \cite{azari2020elsa,park2018maximizing,han2017ese}. Each model has two LSTM layers, and Table~\ref{tab:lstmModels} list the parameters of these models.
\begin{table}[htb]
	\centering
	\caption{LSTM Models used for experiments}
	\begin{tabular}{@{}ccccll@{}}
		\toprule
		\multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Input Size}} & \multicolumn{2}{c}{\textbf{\#Hidden units}} & \multicolumn{2}{l}{\multirow{5}{*}{}} \\ \cmidrule(lr){3-4}
		&                                      & Layer 1              & Layer 2              & \multicolumn{2}{l}{}                  \\ \cmidrule(r){1-4}
		Character Level LM~\cite{azari2020elsa}                              & 65                                   & 128                  & 128                  & \multicolumn{2}{l}{}                  \\ \cmidrule(r){1-4}
		TIMIT-512 \cite{park2018maximizing}                      & 40                                   & 512                  & 512                  & \multicolumn{2}{l}{}                  \\ \cmidrule(r){1-4}
		TIMIT-1024 \cite{han2017ese}                     & 160                                  & 1024                 & 1024                 & \multicolumn{2}{l}{}                  \\ \bottomrule
	\end{tabular}
	\label{tab:lstmModels}
\end{table}
\subsection{Results}
\subsubsection{Off-Chip Memory Access}
Using our FPGA designs (\figurename{~\ref{fig:lstmFpgaDesign}}) that can be configured for different input vector lengths, on-chip buffer sizes, and the number of hidden units we measured the off-chip memory accesses for conventional, TSI-WR and our SACC approach. The hardware implementation was integrated with Xilinx AXI Performance Monitor (APM) IP to log the number of bytes transferred between DDR3 and FPGA design.
We have experimented with different on-chip memory sizes smaller than the size of $R$. \figurename{~\ref{fig:memAccessImprovement}} shows the off-chip memory accesses for two consecutive time steps for conventional, TSI-WR, and SACC approaches.  \figurename{~\ref{fig:memTsiwrComparison} shows the results computed using~\eqref{eq:unalignedAccess} of chapter~\ref{chap:analyticalFw}, and \figurename{~\ref{fig:memAccessAllThree} shows the results measured using our hardware implementation on FPGA (\figurename{~\ref{fig:lstmFpgaDesign}}). 

\begin{figure}[htb!]
	\centering
	\subfloat[]
	{\includegraphics[width=0.35\textwidth]{tsiwrComparisonModified.pdf}
		\label{fig:memTsiwrComparison}}
	\hspace{2.0em}
	\subfloat[]
	{
		\includegraphics[width=0.35\textwidth]{memAccess2TS.pdf}
		\label{fig:memAccessAllThree}
	}
	\caption{Off-chip memory access for matrix-vector multiplication of two consecutive time steps with different on-chip buffer to R matrix size ratio.  (a)  using analytical framework (b) measured on hardware}	
	\label{fig:memAccessImprovement}
	\vspace{-1.0em}	
\end{figure}
If the on-chip buffer size is small compared to the weight matrices, tiles of $R$ are accessed from off-chip memory, replacing the older tiles in the on-chip memory. In conventional approaches, these tiles are not reused for subsequent time step computations, which results in accessing full matrix $R$ every step. For conventional approaches, the off-chip memory accesses remain the same, even if the on-chip mem size is increased, as shown by the horizontal line in \figurename{~\ref{fig:memTsiwrComparison}.
The TSI-WR approach schedules the tiles to reuse the data from the on-chip memory and reduces off-chip memory access. However, the extent of data reuse in the TSI-WR approach depends on the size of overlap between two consecutive tiles, which is decided by the available on-chip buffer size. When the on-chip buffer to R matrix size is close to 48\%, TSI-WR reduces the memory access by $\approx$25\%, as shown in \figurename{~\ref{fig:memAccessImprovement}}. Our approach splits the computations and performs the scheduling of the tiles that reduces the memory accesses by $\approx$50\%, irrespective of the on-chip buffer size.
\subsubsection{Throughput Analysis}
To analyze the throughput variation with different on-chip buffer sizes, we synthesized FPGA designs (\figref{fig:lstmFpgaDesign}) with varying buffer sizes for all three approaches. We recorded the number of cycles (measured\_clock\_cycles) for different FPGA designs over $T$ time steps to evaluate the throughput. Using this data, we computed the throughput using the following formula:
\begin{align}\label{eq:througput}
	\text{Throughput(Mega-MACs/sec)}=\frac{4{\times}N^2{\times}T{\times}{\text {clock\_freq}}}{\text{measured\_clock\_cycles}}
\end{align}
Here, clock\_freq represents the system clock frequency in MHz.
\begin{figure}[htb!]
	\centering
	\subfloat[64KB on-chip buffer size]
	{\includegraphics[width=0.35\textwidth]{througputVsMem_64KB.pdf}
		\label{fig:throughPutVsMem_64}}
	\hspace{2.0em}
	\subfloat[128 KB on-chip buffer size]
	{
		\includegraphics[width=0.35\textwidth]{througputVsMem_128KB.pdf}
		\label{fig:throughPutVsMem_128}
	}
	\caption{Throughput variation with different on-chip buffer/R ratio}	\label{fig:throughputVsMem}
\end{figure}
\figref{fig:throughPutVsMem_64} and \figref{fig:throughPutVsMem_128} compare the throughput for different ratios of on-chip buffer to weight matrix size for 64 and 128 KB on-chip buffer size, respectively. We experimented with different numbers of hidden units ($N$) to vary the $R$ matrix size. Increasing the on-chip buffer to $R$ matrix size ratio results in larger tile sizes and fewer iterations, improving throughput for all three approaches. The TSI-WR approach performs worse than the conventional approach for smaller on-chip buffer/$R$ ratio due to its control logic overhead which overshadows the data reuse benefits. However, for a larger on-chip buffer/$R$ size ratio (e.g., 50\%), the TSI-WR approach outperforms the conventional approach due to better data reuse. Our approach performs better than the other two for all the on-chip buffer to $R$ matrix size ratio due to its significant data-reuse of weight matrix $R$ and small control logic overhead.
For 50\% on-chip buffer to matrix size ratio, our approach improves the throughput by 42\% and 33\% for 64~KB and 41\% and 29\% for 128~KB on-chip buffer size, compared to conventional and TSI-WR approaches, respectively.
\subsubsection{Throughput variation with compute resources}
The memory bandwidth limits the performance of LSTM accelerators. Data reuse reduces the memory bandwidth bottleneck. To observe the impact of data reuse on memory bandwidth, we have implemented multiple FPGA designs with different numbers of compute resources for each of the three approaches. Throughput is measured using \eqref{eq:througput} for each approach. We have compared the results when on-chip buffer to matrix size ratio of 50\%. 
\begin{figure}[htb!]
	\centering
	\subfloat[64 KB on-chip buffer size]
	{\includegraphics[width=0.35\textwidth]{througput_N128.pdf}
		\label{fig:throughPutVsPF_64}
	}
	\hspace{2.0em}
	\subfloat[128 KB on-chip buffer size]
	{
		\includegraphics[width=0.35\textwidth]{througput_N256.pdf}
		\label{fig:throughPutVsPF_128}
	}
	\caption{Throughput variation with compute resources for 50\% on-chip buffer/$R$ ratio}	\label{fig:throughputVsPF}
\end{figure}

\figref{fig:throughputVsPF} compares the throughput of the conventional, TSI-WR and our approach. 
Increasing the number of computing resources does not improve the performance of the conventional approach. TSI-WR approach improves the performance with increasing compute resources as it reuses the on-chip data and improves the operational intensity (operations/byte). However, the improvement in the TSI-WR approach is noticeable only for the large on-chip buffer to $R$ ratios. Our approach (SACC) has better operational intensity (operations/byte). It alleviates the memory bandwidth issue compared to the other two approaches, which improves throughput by increasing the number of parallel resources. The improvement in throughput with the increased number of compute resources using our approach is observed for different on-chip buffer sizes as shown in \figref{fig:throughPutVsPF_64} and \figref{fig:throughPutVsPF_128}.
\subsubsection{Energy Efficiency Improvement}
We used our FPGA implementations to measure the run time for conventional, TSI-WR, and SACC approaches for many time steps. We computed the energy consumption of the designs using the following equation~\cite{tu2017deep}
\begin{align}\label{eq:energyEfficiency}
	E~{=}~P\cdot Time~{+}~\numBytesOffChip\cdot E_{DDR}
\end{align}
where $P$ is the FPGA design power reported by the Vivado synthesis tool, $Time$ is the average run time of the design, $\numBytesOffChip$ is the number of bytes accessed from off-chip memory logged using Xilinx APM IP, and $E_{DDR}$ is the off-chip memory access energy per bit. We have used $E_{DDR}$=70 pJ/bit, a typical value for the DDR3 memory access energy~\cite{6237004}.
\begin{figure}[htb!]
	\centering
	\subfloat[64KB on-chip buffer size]
	{\includegraphics[width=0.35\textwidth]{energyVsMem_64KB.pdf}
		\label{fig:energy_64}
	}
	\hspace{2.0em}
	\subfloat[128 KB on-chip buffer size]
	{
		\includegraphics[width=0.35\textwidth]{energyVsMem_128KB.pdf}
		\label{fig:energy_128}
	}
	\caption{Energy improvement with on-chip buffer size/R}\label{fig:energyVsMem}
\end{figure}
\figref{fig:energy_64} and \figref{fig:energy_128} show the normalized energy efficiency per MAC operation for different on-chip buffer to R matrix size ratios for 64~KB and 128~KB on-chip buffer sizes, respectively. All three approaches observe the improvement with the increase in the on-chip buffer size to matrix size ratio due to a reduction in the control logic execution. TSI-WR performs better than the conventional approach only after the on-chip buffer to R matrix size ratio is greater than 12.5\%. Our approach (SACC ) outperforms the other two approaches for all the cases as it reduces the off-chip memory accesses, which dominates the overall energy consumption. For 50\% on-chip buffer to matrix size ratio, the SACC approach reduces 43\% and 32\% energy for 64~KB on-chip buffer and 48\% and 30\% for 128~KB on-chip buffer size compared to conventional and TSI-WR approach, respectively.

\section{Summary}
Long Short-Term Memory (LSTM) networks are widely used in speech recognition and natural language processing (NLP). With the enormous growth in the number of Edge AI applications, there is a pressing need for efficient execution of these algorithms on edge devices. These edge devices use customized accelerators to meet energy and throughput targets. The key to improving the energy efficiency and throughput of DNN accelerators is to reduce the off-chip memory accesses. This work proposes a novel data reuse approach that reduces the off-chip memory accesses of large weight matrices of RNNs/LSTMs by $\approx$50\% and improves the throughput significantly. Our approach improves the throughput by 55\% and 29\% and reduces the energy consumption by 52\% and 30\% for 12.5\% and 50\% on-chip buffer to matrix size ratio, for 128 KB on-chip buffer size, compared to state-of-the-art TSI-WR approach.
