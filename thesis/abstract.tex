\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}
The recent past has observed explosive growth in the number of Neural Network (NN) based applications. Their ability to solve long outstanding problems with high accuracy has enabled new applications for consumer electronics and embedded devices. Their high accuracy comes at the cost of many computing and memory-intensive operations. A few years back, NN applications primarily ran on the cloud, but now they are preferred to run on the edge device to improve user experience. However, these edge devicesâ€™ limited computing and energy resources pose a significant challenge. Researchers have proposed several custom hardware accelerators to address these challenges. Throughput and energy consumption are the key performance metrics for these edge NN accelerators. These accelerators can meet the performance requirements to some extent, but their energy consumption is still concerning.

Energy-efficient processing of the NNs is paramount for their widespread usage on edge devices. A significant fraction of the energy consumption of these accelerators results from accessing the data from the off-chip memory accesses. The memory bandwidth also limits their throughput. Optimizing the off-chip memory accesses is the key to improving these accelerators' throughput and energy efficiency. This thesis focuses on energy-efficient inferencing of NNs on edge devices by optimizing the off-chip memory accesses at the pre-inferencing step.

Different NNs and layers of a NN exhibit different kinds of data access patterns. Data access patterns vary among the same type of layers within a NN due to varying layer shapes and sizes. There is no optimal global scheme that works for all. Off-chip memory accesses need to be analyzed for each case to find the optimal solution. Extracting the off-chip memory accesses directly from the NN description is difficult as it depends not only on the layer shape but also on the layer partitioning, scheduling, and accelerator's architectural parameters. This thesis proposes an analytical framework that computes the off-chip memory accesses for different types of NN layers and estimates the data movement energy. The analytical framework compares various design choices and trades between them. It guides for finding an energy-efficient solution for different NNs.

Different types of NNs are specialized for solving specific problems. These NNs differ in terms of the number of layers, parameters, data flow, training method, and type of operations. A few approaches can be applied for different NNs to reduce the off-chip memory accesses and works well for several NNs. For example, NNs are error tolerant, and thus representing data in low bit width can bring down energy consumption and storage requirements at the cost of some accuracy loss. However, more sophisticated optimization schemes specific to each NN are required to meet the energy targets. In this thesis, we have covered a range of NNs varying from single-layer, multi-layer feedforward NNs to recurrent NNs. 

To analyze the impact of low resolution on the accuracy of the NN and the benefits of it on energy consumption, we have applied this technique to Self Organizing Maps (SOM). SOM is an example of single-layer feedforward NN used in dimensionality reduction and clustering-related problems. We have implemented a custom pipelined FPGA design to accelerate the SOM processing. The target application is to identify bacterial genomes in clinical settings where optimizing energy and the area is critical. Using the FPGA design, we have analyzed the impact of varying data resolution on the accuracy of the network and energy and area benefits.

Convolution Neural Networks (CNNs) are multi-layer feedforward neural networks and have shown tremendous success in computer vision-related applications like object detection, image classification, and face detection. Loop tiling is commonly applied to partition the layer data into tiles. There are different ways in which layer data can be partitioned and the order in which operations on these tiles can be performed. Due to the large design space, finding the optimal solution is practically difficult by performing measurements on hardware. To address this, we have developed a model to compute the memory access of CLs and FCLs and integrated it with the analytical framework to analyze the memory accesses of different schemes. We have proposed a scheme to determine the optimal layer partitioning and scheduling scheme that maximizes the data reuse while considering the accelerator's architecture parameters, address alignment, and data resolution.

Recurrent Neural Networks (RNN) are another important category of NNs widely used for sequential data processing in speech recognition, natural language processing, and other areas. Long Short-Term Memory networks (LSTMs) are variants of RNNs, designed to handle long-range dependencies. RNNs have recurrent connections and internal states to store information from the past. Due to dependency on previous step computations, LSTM accelerators fail to reuse the data and incur a large volume of data accesses, resulting in high energy consumption. Memory bandwidth also limits the throughput of LSTM accelerators. In this work, we have proposed a novel data reuse scheme to overcome the data-dependency problem of LSTMs that significantly improves the data reuse and the throughput and energy consumption of these accelerators.

Overall, this thesis proposes an analytical framework to compute the memory accesses and analyze the data movement energy of state-of-the-art NNs and proposes novel data-reuse schemes to optimize the off-chip memory accesses of various NNs. This thesis contributes to the state-of-the-art by improving NN accelerators' energy efficiency and throughput during the inference phase.