\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}
Recent past has observed explosive growth in number of Neural Network (NN) based applications. Their ability to solve long outstanding problems with high accuracy have enabled new set of applications for consumer electronics and embedded devices. Their high accuracy comes at the cost of large number of compute and memory intensive operations. Few years back, NN applications were mostly run on the cloud are now prefferred to run on the edge device to mitigate connectivity, network bandwidth, latency and privacy issues. Several latest consumer electronic devices like smart phones, digital camera etc., are already equipped with NN applications. However, limited compute and energy resource on these edge devices pose a significant challenge. Researchers have proposed several custom hardware accelerators to address these issues. Throughput and energy consumption are the key performance metrics for these edge NN accelerators. These accelerators can meet the performance requirements upto some extend but their energy consumption is still concerning. Efficient processing of the NNs is of paramount importance for their widespread usage on edge devices. A significant fraction of the energy consumption of these accelerators results from accessing the data from the off-chip memory accesses. Their performance is also limited by the memory bandwidth. Optimizing the off-chip memory accesses is the key to improve the performance and energy efficiency of these accelerators. This thesis focuses on efficient inferencing of NNs on edge devices by optimizing the off-chip memory accesses at pre-inferencing step.

Computations in NN layers involve accessing multi-dimensional data from off-chip memory. Different NNs and different layers within a NN exhibit different kind of data access pattern. Even, data access pattern varies among the same type of layers within a NN due to varying layer's shape and sizes.  There is no global optimal scheme which works for all. Off-chip memory accesses need to be analyzed for each case to find the optimal solution. 
Extracting the off-chip memory accesses directly from the NN description is difficult as it depends not only on the layer shape but also on the layer partitioning, scheduling and accelerator's architectural parameters. This thesis proposes an analytical framework that computes the off-chip memory accesses for different types of NN layers and estimates the data movement energy for off-chip memory accesses. The analytical framework is used to compare various design choices and trade-off between them. It guides for finding an energy efficient solution for different NNs.

There are different type of NNs which are specialized for solving specific problems. These NNs differ each other in terms of number of layers, number of parameters, data flow, training method and the type of operations. In this thesis we have covered range of NNs varying from single layer, multi-layer feedforward NNs to recurrent NNs. The data access pattern and type of data reuse varies among these NNs. Self Organizing Maps (SOM) is an example of single layer feedforward NN which is an unsupervised learning algorithm, used in dimensionality reduction and clustering related problems. We have implemented a custom pipelined FPGA design to accelerate the SOM processing. The target application is to identify bacterial genome in a clinical environment settings where optimizing energy, area is critical. Using the FPGA design, we have analyzed the impact of varying data resolution on the accuracy of the network and energy and area benefits. 

Convolution Neural Networks (CNNs) are multi layer feedforward neural networks and have shown tremendous success in computer vision related applications like object detection, image classification, face detection etc. CNNs are quintessential example of deep neural networks (DNNs). CNNs primary use a mathematical operation called convolution. There are several convolution layers (CL) and few fully connected layers (FCL) at the end, which are special case of CL. Other layer types like pooling and normalization are also present in CNNs. However processing of CLs dominate the overall computations and energy consumption of CNNs. NN accelerators have small on-chip memory and CNN layer data size doesn't fit in the memory. Loop tiling is applied to partition the layer data into tiles. There are different ways in which layer data can be partitioned and the order in which operation on these tiles can be performed. Finding the optimal way is not trivial here. To address this we have developed a model to compute the memory access of CLs and FCLs and integrated it with the analytical framework to analyze the memory accesses of different schemes. We have proposed a scheme to determine the optimal layer partitioning and scheduling scheme that maximizes the data reuse while considering the accelerator's architecture parameters, address alignment and, data resolution.

Recurrent Neural Networks (RNN) are other important category of NNs widely used for sequential data processing in speech recognition, natural language processing and other areas. They have recurrent connections and have internal states to store the information from the past. Long Short-Term Memory (LSTM) are variants of RNNs, designed to handle long-range dependencies. These networks access the weight matrices repeatedly for large number of time steps. Due to dependency on previous step computations, LSTMs accelerators fails to reuse the data and incur large volume of data accesses, resulting in high energy consumption. Throughput of LSTM accelerators is also limited by memory bandwidth. In this work, we have proposed a data reuse scheme to overcome the data-dependency problem of LSTMs that significantly improve the data reuse and improves the throughput and energy consumption of these accelerators.

Overall, this thesis proposes an analytical framework to compute the memory accesses and analyze the data movement energy of state of the art NNs and proposes novel data-reuse schemes to optimize the off-chip memory accesses. This thesis contributes the state-of-the-art by improving the energy efficiency and throughput of NN accelerators during inference phase.