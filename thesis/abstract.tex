\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}
\begin{comment}
The recent past has observed explosive growth in the number of Neural Network (NN) based applications. Their ability to solve long outstanding problems with high accuracy has enabled new applications for consumer electronics and embedded devices. Their high accuracy comes at the cost of many computing and memory-intensive operations. A few years back, NN applications primarily ran on the cloud, but now they are preferred to run on the edge device to improve user experience. However, these edge devicesâ€™ limited computing and energy resources pose a significant challenge. Researchers have proposed several custom hardware accelerators to address these challenges. Throughput and energy consumption are the key performance metrics for these edge NN accelerators. These accelerators can meet the performance requirements to some extent, but their energy consumption is still concerning.

Energy-efficient processing of the NNs is paramount for their widespread usage on edge devices. A significant fraction of the energy consumption of these accelerators results from accessing the data from the off-chip memory accesses. The memory bandwidth also limits their throughput. Optimizing the off-chip memory accesses is the key to improving these accelerators' throughput and energy efficiency. This thesis focuses on energy-efficient inferencing of NNs on edge devices by optimizing the off-chip memory accesses at the pre-inferencing step.

Different NNs and layers of a NN exhibit different kinds of data access patterns. Data access patterns vary among the same type of layers within a NN due to varying layer shapes and sizes. There is no optimal global scheme that works for all. Off-chip memory accesses need to be analyzed for each case to find the optimal solution. Extracting the off-chip memory accesses directly from the NN description is difficult as it depends not only on the layer shape but also on the layer partitioning, scheduling, and accelerator's architectural parameters. This thesis proposes an analytical framework that computes the off-chip memory accesses for different types of NN layers and estimates the data movement energy. The analytical framework compares various design choices and trades between them. It guides for finding an energy-efficient solution for different NNs.

Different types of NNs are specialized for solving specific problems. These NNs differ in terms of the number of layers, parameters, data flow, training method, and type of operations. A few approaches can be applied for different NNs to reduce the off-chip memory accesses and works well for several NNs. For example, NNs are error tolerant, and thus representing data in low bit width can bring down energy consumption and storage requirements at the cost of some accuracy loss. However, more sophisticated optimization schemes specific to each NN are required to meet the energy targets. In this thesis, we have covered a range of NNs varying from single-layer, multi-layer feedforward NNs to recurrent NNs. 

To analyze the impact of low resolution on the accuracy of the NN and the benefits of it on energy consumption, we have applied this technique to Self Organizing Maps (SOM). SOM is an example of single-layer feedforward NN used in dimensionality reduction and clustering-related problems. We have implemented a custom pipelined FPGA design to accelerate the SOM processing. The target application is to identify bacterial genomes in clinical settings where optimizing energy and the area is critical. Using the FPGA design, we have analyzed the impact of varying data resolution on the accuracy of the network and energy and area benefits.

Convolution Neural Networks (CNNs) are multi-layer feedforward neural networks and have shown tremendous success in computer vision-related applications like object detection, image classification, and face detection. Loop tiling is commonly applied to partition the layer data into tiles. There are different ways in which layer data can be partitioned and the order in which operations on these tiles can be performed. Due to the large design space, finding the optimal solution is practically difficult by performing measurements on hardware. To address this, we have developed a model to compute the memory access of CLs and FCLs and integrated it with the analytical framework to analyze the memory accesses of different schemes. We have proposed a scheme to determine the optimal layer partitioning and scheduling scheme that maximizes the data reuse while considering the accelerator's architecture parameters, address alignment, and data resolution.

Recurrent Neural Networks (RNN) are another important category of NNs widely used for sequential data processing in speech recognition, natural language processing, and other areas. Long Short-Term Memory networks (LSTMs) are variants of RNNs, designed to handle long-range dependencies. RNNs have recurrent connections and internal states to store information from the past. Due to dependency on previous step computations, LSTM accelerators fail to reuse the data and incur a large volume of data accesses, resulting in high energy consumption. Memory bandwidth also limits the throughput of LSTM accelerators. In this work, we have proposed a novel data reuse scheme to overcome the data-dependency problem of LSTMs that significantly improves the data reuse and the throughput and energy consumption of these accelerators.

In conclusion, this thesis contributes to the optimization of NN accelerators on parallel architectures, addressing energy efficiency and throughput challenges. The analytical framework and data-reuse schemes provide valuable insights for designing energy-efficient NN inferencing on edge devices.
\end{comment}
In recent years, Neural Networks (NNs) have experienced tremendous growth in applications across diverse fields, revolutionizing industries such as healthcare, agriculture, surveillance, and autonomous systems. The availability of large datasets and powerful computing systems, coupled with user-friendly libraries like Tensorflow and PyTorch, has enabled NNs to achieve human-like performance. As a result, NN-based applications have become an integral part of our daily lives, from face recognition and chatbots to shopping recommendations and medical diagnosis.

This Ph.D. thesis focuses on optimizing the inferencing phase of NNs for edge devices with limited computing resources and tight energy budgets. Edge devices, such as smartphones and wearable devices, are now widely equipped with NN applications, necessitating energy-efficient solutions for improved user experiences and privacy protection. The study encompasses three classes of NNs: Self Organizing Maps (SOMs), Convolutional Neural Networks (CNNs), and Recurrent Neural Networks (RNNs), each posing unique challenges in terms of data reuse and energy efficiency.

The thesis begins by introducing the neural network model, illustrating the structure and different types of NNs, including feedforward, recurrent, and deep neural networks. The development and deployment phases of NN applications are also explained, emphasizing the significance of optimizing the inferencing phase for edge devices.

In the context of CNNs, the research focuses on optimizing data reuse through partitioning and scheduling schemes. An analytical framework is developed to estimate off-chip memory accesses, considering architectural constraints and data shape. The framework enables the comparison of various partitioning and scheduling approaches, facilitating the discovery of optimal solutions to improve energy efficiency and throughput for different CNN layers.

For RNNs, which present additional challenges due to their dependency on previous time-step computations, a novel data reuse approach is proposed. This approach efficiently reuses weights of large matrices for consecutive time steps, irrespective of on-chip memory size. FPGA implementations of Long-Short Term Memory Network (LSTM) accelerators demonstrate the effectiveness of this approach in enhancing energy efficiency and throughput for popular LSTM models.

For SOMs, the study explores the impact of quantization techniques on accuracy and energy efficiency. A custom semi-systolic array design is devised, and a trade-off between NN accuracy and energy consumption is analyzed for different bit-width implementations. The study analyzes the benefits and trade-offs of using different bit resolutions for SOMs, catering to energy-constrained systems where area, power, and performance are critical considerations.

In conclusion, this Ph.D. thesis advances the state-of-the-art in energy-efficient acceleration of modern NNs for edge devices. By proposing novel data reuse techniques and analytical frameworks, the research provides valuable insights for improving the inferencing phase, thus contributing to the widespread adoption of NN-based applications in energy-constrained environments. The work also opens new avenues for future research, pushing the boundaries of NN optimization and edge AI applications.