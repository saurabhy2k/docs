\graphicspath{{./Ch1-Introduction/images/}}

\chapter{Introduction} \label{chap:introduction}
\begin{comment}
//clearly come out what are you doing in this thesis and what are the contributions
//what field you are targetting, scope and motivation, what problems, to justify your contributions.
1. About AI/ML/NN/DNNs: Applications, Domains, Growth, Type of NNs, Training and inference, 

2. Modern DNNs, architectures and popular NNs

3. Example: Number of computations, parameters: Illustrate compute and memory intensive operations.
Number of layer

4. Edge vs. Cloud Computing. 

5. Edge AI Challenges

6. About the Architectures for Edge AI Accelerators: Give wider scope of these architectures, and specific type of architectures. Trade-offs.

7. Performance bottlenecks for edge AI: Memory and Throughput bottlenecks.
Memory accesses are the key botllenecks for these edge AI devices. 

6. Summary and Outline of the Thesis.

\end{comment}

The past few years have seen exponential growth in Neural Network (NN) based applications. NN applications are widely used in healthcare, agriculture, road safety, surveillance, defense, number plate identification, medical diagnosis, autonomous driving, recommender systems, and many more. Modern computing systems capable of storing and processing large volumes of data, and the availability of big data sets due to digitization, have enabled NNs to achieve human-like performance, which was not possible a few decades ago. Their growth was also accelerated by the software libraries like Tensorflow and PyTorch that provide ease of development. 

Today, NN-based applications have penetrated our daily lives, e.g., face recognition for authentication, chatbots, shopping recommendations, and photo tagging. Their unprecedented success in solving complicated real-world problems has increased their usage in embedded systems ranging from smartphones and tablets to wearable devices. NNs are also used in the clinical environment, e.g., in bacterial identification and tumor detection. Several of these devices need to be mobile, battery-operated and should have lightweight. Often, such systems have limited computing resources and tight energy-budget. With time, NNs are growing in size to solve complicated problems and improve accuracy; this trend is expected to continue for several years. 

\section{Background}
Neural Networks are machine learning algorithms inspired by processing mechanisms in the human brain. They can learn from the data using a training process without being programmed explicitly. Inspired by the brain, NNs consist of several neurons connected and organized as layers. There can be several hidden layers in a NN. \figurename{~\ref{fig:simpleNN}} shows a simple NN example. 
\begin{figure}[!htb]
	\centering
	\captionsetup{font=sf}
	\includegraphics[width=0.3\textwidth]{simpleFFNN}
	\caption{Example of simple Neural Network.}
	\label{fig:simpleNN}
\end{figure}

Several classes of NNs exist that differ in the number of neurons, connections between them, and training method. Broadly they can be classified as feedforward neural networks (FFNNs) and recurrent neural networks (RNNs) based on the flow on the flow of data between layers. Convolutional Neural Networks (CNNs) and Long short-term memory networks (LSTMs) are quintessential examples of FFNNs and RNNs, respectively. CNNs are mainly used in computer vision-related applications like image classification and object detection, and LSTMs are used in sequential data processing applications like speech recognition and natural language processing (NLP). 

Modern DNN architectures are very deep and involve many layers and millions of parameters. 
Multilayer NNs with more than three layers are referred to as deep neural networks (DNNs). These DNNs are capable of learning complex functions from the data. Besides DNNs, there are single-layer NNs that consist of just two layers - an input and an output layer. Only the output layer performs the computation in this case. Self-Organizing Maps (SOMs) are examples of single-layer NNs and are used in dimensionality reduction and clustering applications. 

\subsection{Phases of NN Applications: Development to Deployment}
NNs need not be programmed explicitly, and they learn from the raw data to provide solutions to real-world problems. For example, NNs first learn to classify an object in an image, in which several example images are provided. This learning process is referred to as training. Once trained, the NN can estimate the output for a new input. Subsequently, the trained NNs are used in applications to estimate the new input's output. This phase is referred to as inference.

\figurename{~\ref{fig:workFlow}} shows difference phases of NNs from development to deployment. The development phase is usually performed on desktop/laptop machines using deep neural network frameworks, e.g., PyTorch and TensorFlow. 
\begin{figure}[!htb]
	\centering
	\captionsetup{font=sf}
	\includegraphics[width=0.8\textwidth]{WorkFlow}
	\caption{Work Flow for Neural Networks.}
	\label{fig:workFlow}
\end{figure}
After the development phase, NNs undergo a training phase where they learn from examples. These DNNs require large training data sets for learning and must go through multiple iterations to achieve the desired accuracy. Training of NNs involves updating weights and biases using large input samples. It is a repetitive process until the desired accuracy is achieved. Training a DNN may last from a few hours to several weeks. Hence training of DNNs is generally performed on high-performance systems, typically using GPUs. 

After the training phase, trained models are used in applications for inferencing, where the deployment platforms may range from cloud to embedded devices. Inference phase computations also involve millions of computations and access large volumes of data. A popular CNN, VGG16, performs 15.47 GMAC operations and accesses 138 million parameters for a single inference. If the NNs are deployed on the cloud, the throughput and energy demands can be met using high-performance systems like GPU. However,  deployment on edge devices is challenging due to limited energy and computing resources. Hence optimizing these models before inferencing on edge devices is a must. In this work, we focused on optimizing NN models before deployment on edge devices for inferencing. This phase is shown as \textbf{Preprocessing and Optimization} block in \figurename{~\ref{fig:workFlow}}.

\section{Edge AI: Inferencing on Edge Devices}
Edge devices are battery-operated with limited resources and a tight energy budget. A few years back, edge devices were primarily used to collect real-world data, and inferences were performed on the cloud.
There is a growing trend of shifting the processing of these DNN applications from the cloud to edge devices near the sensors as it improves the user experience, eliminates network bandwidth issues, and improves privacy and security. 

The recent growth of deep learning, advancement in deep learning tools, and recent research in the field of efficient edge AI accelerators have enabled several intelligent applications for consumer and edge devices. Today we see an explosion of applications using deep learning algorithms in consumer electronic devices.
Manufacturers are shifting the processing of NNs from cloud to edge devices like smartphones and tablets. 

However, DNN inferencing on edge devices is challenging. \figurename{~\ref{fig:edgeAIChallenges}} shows the key challenges for Edge AI devices. Energy efficiency and throughput are the two most important metrics for edge devices. While energy efficiency is paramount for longer battery time, high throughput is desired for better user-response time. Efficient processing of DNNs inferencing on edge devices is critical for their widespread usage. 

\begin{figure}[!htb]
	\centering
	\captionsetup{font=sf}
	\includegraphics[width=0.6\textwidth]{edgeAIChallenges}
	\caption{Key challenges for Edge-AI Accelerator}
	\label{fig:edgeAIChallenges}
\end{figure}
\subsection{NN Accelerators}
Edge devices use customized NN accelerators to meet energy and throughput demands. \figurename{~\ref{fig:typicalDNNAccelerator}} shows a typical DNN accelerator architecture, which consists of an off-chip memory and an accelerator chip. An accelerator chip mainly consists of an on-chip memory of a few hundred KBs and an array of Processing Elements (PEs). The accelerator system has multiple memory levels: off-chip memory, on-chip memory, and the registers inside the PEs. Each memory level has a different storage capacity, access latency, and energy costs. The memory access energy from off-chip memory is up to two orders of magnitude higher than a PE computation operation~\cite{Chen2016EyerissAS}. More than 80\% of the overall energy consumption of these accelerators is due to off-chip memory accesses~\cite{chen2014diannao}. Therefore, reducing the off-chip memory is the key to improving the throughput and energy efficiency of DNN accelerators. Most recent research has focused on reducing off-chip memory accesses.
\begin{figure}[!htb]
	\centering
	\captionsetup{font=sf}
	\includegraphics[width=0.7\textwidth]{typicalDNNAccelerator}
    \caption{Typical DNN accelerator architecture.}
   	\label{fig:typicalDNNAccelerator}
    \vspace{1.0em}
\end{figure}

In this thesis, we desired to improve edge NN accelerators' performance and energy efficiency during the inference phase. We aimed to provide energy-efficient solutions for NN accelerators by reducing the off-chip memory accesses by applying novel data reuse schemes and representing data in low resolution.

\section{Previous Work}
Several FPGA~\cite{zhang2015optimizing,wei2019overcoming,gokhale2014240,8742284,gupta2015deep,alwani2016fused}, GPU~\cite{chetlur2014cudnn} and ASIC~\cite{Chen2016EyerissAS,chen2014diannao,chen2014dadiannao,du2015shidiannao} accelerators have been proposed to meet the performance and energy targets. These accelerators apply different techniques to speed up operations. Since memory accesses dominate the performance and energy of these accelerators, recent researches focus on reducing the off-chip memory accesses~\cite{chen2014diannao,chen2016eyeriss,zhang2015optimizing}. Some approaches~\cite{lee2016fpga, rybalkin2018finn, ferreira2016fpga} used on-chip memory to store all the weights. Sizes of weights in modern NN models can be several MBs. These approaches are not scalable and effective only for small NN models. 
Works that aim to reduce the off-chip memory accesses of NN accelerators can be classified into two broad categories, as shown in Figure~\ref{fig:previousWorkClassification}. 
\begin{figure}[!htb]
	\centering
	\captionsetup{font=sf}
	\includegraphics[width=0.5\textwidth]{previousWorkClassification}
	\caption{Broad Classification of previous works for improving the performance of DNN accelerators.}
	\label{fig:previousWorkClassification}
\end{figure}

One category of work exploits error-tolerance and redundancy in NNs using quantization, compression, and pruning techniques to reduce the precision, the number of operations, and models' size~\cite{ferreira2016fpga,wang2018c,chang2015recurrent,han2017ese,lee2016fpga}. With reduced precision, the storage requirement and memory accesses reduce proportionally and improve energy efficiency~\cite{sze2017efficient}. Quantization and pruning techniques result in reduced NN model sizes. The reduced model for shallow NNs may fit into the on-chip memory and thus eliminate the off-chip memory bandwidth bottleneck. However, quantization and pruning approaches impact the accuracy of the networks and may only be suitable where accuracy can be compromised. Also, the number of parameters in modern DNNs is significantly large. For these DNNs, besides quantization and pruning, additional techniques are required to reduce the off-chip memory accesses further.

Data-reuse schemes are the other line of approach that does not affect the accuracy of the network. The data-reuse schemes are orthogonal to the quantization techniques and can be combined to reduce off-chip memory accesses further. These schemes aim to reduce repeated off-chip accesses to the same NN coefficients when the entire set of NN coefficients does not fit in the on-chip memory. This is quite effective for many modern DNNs (e.g., CNNs, RNNs) with a significantly large number of parameters~\cite{zhang2015optimizing, Li2018SmartShuttleOO,que2019efficient,park2020time}. One popular scheme which reuses weights is batch processing~\cite{que2019efficient}. During the inference phase, all the inputs use the same weights. In the batch processing scheme, inputs are grouped as a batch and processed to reuse the weights from the on-chip memory. Increasing the batch size improves the weights reuse but increases the latency, which is not desirable. Secondly, the batch processing scheme only improves the weights reuse, and other layer data types do not get the benefit.

Another prominent data reuse technique is data partitioning and scheduling~\cite{zhang2015optimizing,zhang2015optimizing}. In this technique, the data is partitioned into tiles, and operations are scheduled so that data can be accessed from the on-chip memory as far as possible. For a given DNN, there are numerous ways of data partitioning and scheduling, offering different extents of data reuse. Loop tiling is applied to partition the layer data, a well-known compiler technique. Conventional problems apply loop tiling where equal data-reuse opportunities exist in all the data dimensions. However, in NN layers, there are multiple types of data reuse, and the extent of each data reuse varies with different dimensions. Increasing tile dimensions to improve one type of data reuse reduces other types. It is essential to consider all the possible types of data reuses of a layer to find the optimal solution. Deciding on the optimal partitioning of NN layers is more challenging than conventional problems. Choosing an optimal way here is non-trivial.

We have explored both approaches in this work and contributed some new ideas. We have applied quantization techniques on Self Organizing Maps (SOMs), a single layer FFNN, to analyze the impact on NN accuracy and benefits on improving energy efficiency. We hand-crafted a custom semi-systolic array design for different bit width implementations to analyze the accuracy versus energy trade-off for SOMs. We also proposed novel data reuse approaches for multilayer feedforward and recurrent NNs. 

To determine the optimal scheme, estimating the off-chip memory accesses is essential. From the description of the NN layer, it is difficult to estimate the off-chip memory accesses. Several factors impact the off-chip memory accesses of the NN layer. In this work, we have developed an analytical framework that computes the off-chip memory accesses of 3D data partitioned into small tiles. The analytical framework considers data shape, tile dimensions, accelerator architecture parameters, and data resolution to compute the off-chip memory accesses.

We have analyzed the benefits of different partitioning and scheduling schemes for popular CNNs. There are several possible ways to partition the layer data and schedule the operations. Finding the optimal partitioning and scheduling scheme by performing the measurements on the hardware is time-consuming, and vast search space makes it practically impossible. We expressed determining the optimal solution that minimizes off-chip memory accesses of a NN layer as a constraint optimization problem. We have developed a model and integrated it with the analytical framework for computing the off-chip memory accesses of CNN layers. We have analyzed the impact of the accelerator's architectural parameters, e.g., bus widths, address alignments, and on-chip memory sizes on off-chip memory accesses of CNN layers. 

We implemented the hardware design for memory-intensive CNN layers on FPGA to measure the off-chip memory accesses, latency, run-time, and design power. The implementation is configurable for layer shapes, tile dimensions, on-chip memory sizes, bus width, and data resolution. We measured the energy efficiency and throughput of different approaches using the FPGA design and validated the results of the analytical framework.

RNNs computations involve repeated access to the same weight matrices for long sequences. Dependency on previous time step computations and limited on-chip memory of accelerators results in large off-chip memory accesses. To overcome this, we proposed a novel data reuse approach for RNNs that significantly reduces the off-chip memory accesses by reusing the weights for two consecutive time steps. A key characteristic of the proposed approach is that it is independent of the accelerator's on-chip memory size, making it suitable for LSTM accelerators with small on-chip memory. We implemented the hardware design for LSTMs on FPGA for the proposed, conventional, and state-of-the-art data-reuse approaches. Using the FPGA designs, we estimated the energy efficiency and throughput and demonstrated the efficacy of our approach for popular LSTM models.

\section{Summary and Outline of the Thesis} 
Recent advancements in NNs have enabled them to solve several real-world problems, which researchers long ago considered difficult. Due to their high accuracy, they are now being used in many domains. The last few years have witnessed enormous growth in the number of intelligent applications targeted for edge devices. However, resource and energy-constrained edge devices pose a significant challenge to the efficient processing of NN applications. Hence there is a pressing need for energy-efficient execution of NNs applications for their wide acceptance. In the NN inference phase, a large amount of energy consumption results from expensive off-chip memory accesses. Optimizing off-chip memory accesses is key to improving these edge devices' performance and energy efficiency.

Previous research has shown that for data partitioning and scheduling, making a choice independently for each layer of a NN is better than making a common choice for the entire NN because of the different shapes of various layers. We observe that the choice of optimal data partitioning and scheduling depends on the shape of a layer and architectural parameters. We present an analytical framework in Chapter~\ref{chap:analyticalFw} that quantifies the off-chip memory accesses for DNN layers of varying shapes, taking into account the architectural constraints. It helps compare different data partitioning and scheduling schemes to explore the large design space to find the optimal solution for improving the energy and throughput. 

Based on the above analytical framework, in Chapter~\ref{chap:CNN}, we propose a data reuse approach that considers the architectural parameters and determines the optimal partitioning and scheduling scheme to minimize the off-chip memory access of CNN layers. We demonstrate the efficacy of our partitioning and adaptive scheduling approach on the compute and memory-intensive CNN layers. 

Chapter 4 proposes a novel data reuse approach to improve the throughput and energy efficiency of state-of-the-art recurrent neural networks (RNNs). The proposed approach splits the computations and combines them in a way that significantly reduces the off-chip memory accesses of large matrices. We measure the design power and memory accesses on FPGA implementation of Long-Short Term Memory Network (LSTM) accelerators and show our approach's energy and throughput improvements.

We analyze the effect of using different bit resolutions on the accuracy of a NN and the benefits of it for self-organizing maps (SOMs) for designing energy-constrained systems where the area, power, and performance are of critical importance in Chapter 5. Using an efficient implementation of SOM design on FPGA, which can be configured for different bit resolutions, we show performance comparison for different data precisions. 

The work done in this thesis improves the state-of-the-art of energy efficient execution of modern NNs and also gives directions to future research. In chapter 6, we discuss these new research directions together with the conclusion of our work.